{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-parameter Amp Model Training\n",
    "\n",
    "## Step 1: Get Data\n",
    "\n",
    "- Download the wave file to be played into the amp: [Proteus_Capture.wav](https://github.com/akaztp/Automated-GuitarAmpModelling-MultiParameter/raw/refs/heads/master/Data/Proteus_Capture.wav)\n",
    "- Record the output from the amp with different values of its effects (i.e., parameters). Wave files are expected to be in 48kHz. Sample format can be any, but the higher the better like PCM24 or FP32.\n",
    "- Generate a JSON file named \"configuration.json\" with the description of the recorded wave files\n",
    "```JSON\n",
    "{\n",
    "    \"Number of Parameters\": 2,\n",
    "    \"Data Sets\":[\n",
    "    {\n",
    "        \"Parameters\": [ 0.0, 0.0 ],\n",
    "        \"TrainingClean\": \"input.wav\",\n",
    "        \"TrainingTarget\": \"0_0_output.wav\"\n",
    "    },\n",
    "    {\n",
    "        \"Parameters\": [ 0.0, 0.25 ],\n",
    "        \"TrainingClean\": \"input.wav\",\n",
    "        \"TrainingTarget\": \"0_0.25_output.wav\"\n",
    "    },\n",
    "    ...\n",
    "    ]\n",
    "}\n",
    "```\n",
    "Note: The parameters values are between 0.0 and 1.0\n",
    "- Build a zip file with the \"configuration.json\" and the other wave files flat in the top level of the archive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@markdown ## Step 2: Setup environment, upload and extract training data\n",
    "%cd /content\n",
    "!rm -rf Automated-GuitarAmpModelling-MultiParameter\n",
    "!git clone https://github.com/akaztp/Automated-GuitarAmpModelling-MultiParameter\n",
    "%cd /content/Automated-GuitarAmpModelling-MultiParameter\n",
    "!git submodule update --init --recursive\n",
    "import os\n",
     "# Set environment variable for protobuf to use pure-Python implementation\n",
     "# This is slower but avoids compatibility issues with newer protobuf versions\n",
     "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\n",
    "!pip install -r requirements.txt --ignore-requires-python\n",
    "\n",
    "# GPU Check\n",
    "import torch\n",
    "print(\"GPU Available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "from google.colab import files\n",
    "print(\"Please upload a ZIP file containing training configuration and wavefiles.\")\n",
    "uploaded = files.upload()\n",
    "if uploaded:\n",
    "    for filename in uploaded.keys():\n",
    "            if filename.endswith('.zip'):\n",
    "                !unzip {filename} -d input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@markdown ## Step 3: Prepare wav files for training\n",
    "!python prep_wav2.py model -p \"./input/configuration.json\" # --normalize true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@markdown ## Step 4: Train with experiments and report\n",
    "epochs = 200 #@param {type: \"number\"}\n",
    "hidden_size1 = 15 #@param {type: \"number\"}\n",
    "hidden_size2 = 20 #@param {type: \"number\"}\n",
    "hidden_size3 = 30 #@param {type: \"number\"}\n",
    "hidden_size4 = 40 #@param {type: \"number\"}\n",
    "hidden_size5 = 50 #@param {type: \"number\"}\n",
    "#@markdown Set any size to 0 to exclude it from the experiment\n",
    "\n",
    "# Filter out zero values\n",
    "hidden_sizes = [size for size in [hidden_size1, hidden_size2, hidden_size3, hidden_size4, hidden_size5] if size > 0]\n",
    "\n",
    "# Import the experiment class\n",
    "from hidden_size_experiment import HiddenSizeExperiment\n",
    "\n",
    "# Create an experiment with custom hidden sizes\n",
    "experiment = HiddenSizeExperiment(\n",
    "    hidden_sizes=hidden_sizes,  # Your custom hidden sizes\n",
    "    epochs=epochs,              # Number of epochs per model\n",
    ")\n",
    "\n",
    "# Run the experiment\n",
    "results = experiment.run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@markdown ## Step 5: Plot the result error-to-signal (optional)\n",
    "hidden_size = 40 #@param {type: \"number\"}\n",
    "\n",
    "# The plots will be generated in the Results/modelName/ directory\n",
    "output = !python plot.py model --model_name hs_{hidden_size} --pred_wav test_out_best.wav\n",
    "\n",
    "from IPython.display import display, Image\n",
    "import re\n",
    "\n",
    "match = re.search(r'NOTEBOOK_DISPLAY_IMAGES:(.*?),(.*?)$', '\\n'.join(output), re.MULTILINE)\n",
    "\n",
    "if match:\n",
    "    comparison_path = match.group(1)\n",
    "    detail_path = match.group(2)\n",
    "    \n",
    "    # Display the images\n",
    "    print(\"\\nSignal Comparison Plot:\")\n",
    "    display(Image(comparison_path))\n",
    "    \n",
    "    print(\"\\nDetail Signal Comparison Plot:\")\n",
    "    display(Image(detail_path))\n",
    "else:\n",
    "    print(\"No image paths found in the output.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@markdown ## Step 6: Download the results in a zip file.\n",
    "hidden_size = 40 #@param {type: \"number\"}\n",
    "\n",
    "!if [ -f results_hs_{hidden_size}.zip ]; then rm results_hs_{hidden_size}.zip; fi\n",
    "!cd Results/hs_{hidden_size} && zip ../../results_hs_{hidden_size}.zip *\n",
    "!cd ../../\n",
    "\n",
    "# Download the zip file\n",
    "from google.colab import files\n",
    "files.download(f'results_hs_{hidden_size}.zip')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7:  Use the model in the plugin\n",
    "The model file to use in the plugin is \"model_best.json\" from the downloaded archive.\n",
    "For more than one parameter, identify the effect names and their order. This info is needed in the plugin.\n",
    "Suggestion: rename the model json file to have the effects names, like \"gibson-saturn_model_bass-treble.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes and troubleshooting\n",
    "\n",
    "- Make sure this notebook is connected and using GPU. There is a button \"Connect\" in the top right corner.  You have a limited number of consecutive GPU hours with the free version of Colab, but this will reset in a day or so. You might need to buy compute credits for more intensive work.\n",
    "- The \"Train\" step can end before the specified number of epochs if the model stops improving enough. This step can also be stopped at any time.\n",
    "- A model can be refined in more than one run of the \"Train\" step, by specifying more \"epochs\" and run the \"Train\" step again.\n",
    "- The final loss values during training should be less than 0.08 to be considered successful. A loss of 0.02 or less is ideal. Higher than 0.10 indicates difficulty in training a model from the device, but note that mic'd amps will typically have a higher loss due to the complexities of the cab/mic. If the loss remains at 0.75 or higher, this could indicate a problem with the out.wav file, either from audio misalignment or error during the upload to Colab.\n",
    "- For Parameterized Capture of an amp knob, if the device knob at 0% means there is no volume out of your device (such as a gain knob on an amplifier), then you should raise it slightly until your hear volume out. The model training doesn't do well if one of the target wavefiles is silent.\n",
    "\n",
    "\n",
    "# Factors to Consider When Choosing Hidden Size\n",
    "\n",
    "- Complexity of the Audio Transformation:\n",
    "  - Higher complexity (like modeling complex distortion effects with many parameters) requires larger hidden sizes (40+)\n",
    "  - Simpler transformations can work with smaller hidden sizes (20 or less)\n",
    "- Available Training Data:\n",
    "  - More training data supports larger hidden sizes without overfitting\n",
    "  - Limited data may require smaller hidden sizes to prevent overfitting\n",
    "- Computational Resources:\n",
    "  - Larger hidden sizes require more memory and computational power\n",
    "  - Consider your hardware limitations during both training and inference\n",
    "- Real-time Performance Requirements:\n",
    "  - If the model will be used in real-time applications, smaller hidden sizes will be more efficient\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Multi-Parameter Amp Model Training",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
